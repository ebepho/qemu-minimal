#!/bin/bash
#
# virt-install-ubuntu
#
# (C) Stephen Bates <sbates@raithlin>
#
# A simple script generate a Ubuntu Bionic Bison or Focal Fossa or
# later based VM.
#
# Bionic
# ------
#
# Note that for Bionic in order to get the console to work you need
# to add console=ttyS0 to the kernel command line params inside the
# guest. Provide the name and a kickstart file (which can be used to
# completely automate the install process.
#
# Note that for Eideticom installs you also want to pass in the LDAP
# files via the INITD_INJECT argument. An example call that works well
# on Eideticom systems is (note this is obsolete as Eideticom has
# moved to AD):
#
# KS=eideticom-vm.ks NAME=stephen INITRD_INJECT=" --initrd-inject \
#    /home/users/sbates/ldap/ldap.secret \
#    --initrd-inject /home/users/sbates/ldap/ldap.conf "\
#    ./virt-install-bionic
#
# and a good generic call for Bionic might be
#
# RELEASE=bionic KS=generic-vm.ks NAME=stephen ./virt-install-ubuntu
#
# Focal (or Later)
# ----------------
#
# Note that for Focal you need the cloud-init as well as
# cloud-image-utils  and for focal a good generic call would be:
#
# RELEASE=focal NAME=stephen ./virt-install-ubuntu
#
# This creates a system with an user called ubuntu and the password
# change who has full sudo access to the system. This also sets up the
# networking as NAT to the host and you can then ssh into the machine
# using the ubuntu user.
#
# See [1] for a good HOWTO for cloud-init.
#
# [1] https://fabianlee.org/2020/02/23/kvm-testing-cloud-init-locally-using-kvm-for-an-ubuntu-cloud-image/
#
# Note that after running this script you might want to remove the
# cloud-init disk from the VM. See [1] for info on how to do that.

# Note that the variables ARCH, SSH_KEY_FILE, USERNAME, PASS,
# NOAUTOCONSOLE and PACKAGES only apply to focal or later.
#
# PACKAGES is a file of packages to be installed via cloud-init. A
# couple of my favourite collections can be found in the top-level
# packages.d folder.
#
# FILESYSTEM can be a directory on the host that is then shared into
# the guest via the tag "hostfs" and the virtiofs driver. To access
# this folder ensure virtiofs is enabled in your guest kernel (it is
# in default Ubuntu 22.04) and run:
#
# sudo mount -t virtiofs hostfs <mount location on guest>
#
# Add something like this to the guest /etc/fstab to make this happen
# every reboot.
#
# BRIDGE can be set to specify a network bridge on the host to attach
# the VM too. This replaces the NAT-based default. The user needs to
# ensure that the bridge is correctly setup for network access.

NAME=${NAME:-qemu-minimal}
KS=${KS:-none}
INITRD_INJECT=${INITRD_INJECT:-}
SIZE=${SIZE:-32}
RELEASE=${RELEASE:-noble}
ARCH=${ARCH:-amd64}
SSH_KEY_FILE=${SSH_KEY_FILE:-~/.ssh/id_rsa.pub}
NOAUTOCONSOLE=${NOAUTOCONSOLE:-false}
USERNAME=${USERNAME:-ubuntu}
PASS=${PASS:-password}
PACKAGES=${PACKAGES:-../packages.d/packages-default}
FILESYSTEM=${FILESYSTEM:-none}
BRIDGE=${BRIDGE:-none}

if [ $RELEASE == "bionic" ]; then

    echo ${NAME} > hostname.tmp
    INITRD_INJECT+=" --initrd-inject ./hostname.tmp "

    if [ $KS != "none" ]; then
	INITRD_INJECT+=" --initrd-inject ./${KS} "
	KSF="ks=file:/${KS}"
    else
	KS=
	KSF=
    fi

    virt-install \
	--connect qemu:///system \
	--name ${NAME} \
	--memory 1024 \
	--vcpus 1 \
	--virt-type kvm \
	--disk size=${SIZE} \
	--graphics none \
	--network bridge=virbr0 \
	--console pty,target_type=serial \
	--location http://archive.ubuntu.com/ubuntu/dists/bionic/main/installer-amd64/ \
	${INITRD_INJECT} \
	--extra-args ''${KSF}' console=ttyS0,115200n8 serial'

    exit 0

else

    if [ ${RELEASE} == "focal" ]; then
	OSINFO="ubuntufocal"
    elif [ ${RELEASE} == "jammy" ]; then
	OSINFO="ubuntujammy"
    elif [ ${RELEASE} == "noble" ]; then
       OSINFO="ubuntunoble"
    else
	echo "RELEASE ${RELEASE} is not supported yet!"
	exit -1
    fi
fi

  # Focal and above prefers us to use cloud images and
  # cloud-init. Download the focal cloud image and set it up using a
  # cloud-config file. Create am ubuntu user with password-less root
  # access.
  #
  # We use vol-create and vol-upload to add the disk images to the
  # libvirt images folder.

set -e

cleanup() {
    rm -f cloud-config-${NAME} network-config-${NAME} ${NAME}.qcow2 \
   ${NAME}-seed.qcow2
}
trap cleanup SIGINT ERR EXIT

if [ ! -f  ${RELEASE}-server-cloudimg-${ARCH}.img ]; then
    wget https://cloud-images.ubuntu.com/${RELEASE}/current/${RELEASE}-server-cloudimg-${ARCH}.img
fi
cp ${RELEASE}-server-cloudimg-amd64.img ${NAME}.qcow2
qemu-img resize ${NAME}.qcow2 ${SIZE}G

if [ ! -f $SSH_KEY_FILE ]; then
     echo "SSH_KEY_FILE ${SSH_KEY_FILE} does not exist!"
     exit 1
fi

if [ ${PACKAGES} != "none" ]; then
    if [ -f ${PACKAGES} ]; then
	PACKAGES=$(<${PACKAGES})
    else
	echo "Package manifest file ${PACKAGES} does not exist!"
	exit 1
    fi
else
    PACKAGES=
fi

cat << EOF > cloud-config-${NAME}
#cloud-config
hostname: ${NAME}
disable_root: true
ssh_pwauth: true
users:
  - name: ${USERNAME}
    plain_text_passwd: '${PASS}'
    lock_passwd: false
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: users, admin
    shell: /bin/bash
    ssh_authorized_keys: |
      $(sed -z 's|\n|\n      |g' ${SSH_KEY_FILE})
timezone: America/Edmonton
ntp:
  enabled: true"
packages:
${PACKAGES}
power_state:
  delay: now
  mode: poweroff
  message: Shutting down
  timeout: 2
  condition: true
EOF

cat << EOF > network-config-${NAME}
version: 2
ethernets:
  eth0:
    match:
      name: en*
    dhcp4: true
    # default libvirt network
    gateway4: 192.168.122.1
    nameservers:
      addresses: [ 192.168.122.1,8.8.8.8 ]
EOF

create_pool() {
    rc=0
    `virsh pool-info $1 > /dev/null 2>&1` || rc=$?
    if [[ $rc -ne 0 ]]; then
	virsh pool-define-as $1 dir --target /var/lib/libvirt/images/
	virsh pool-autostart $1
	virsh pool-start $1
    fi
}

remove_vol() {
    rc=0
    `virsh vol-info --pool default $1 > /dev/null 2>&1` || rc=$?
    if [[ $rc -eq 0 ]]; then
	virsh vol-delete --pool default $1
    fi
}

create_pool default

remove_vol ${NAME}.qcow2
virsh vol-create-as default ${NAME}.qcow2 ${SIZE}G --format qcow2
virsh vol-upload --pool default ${NAME}.qcow2 ${NAME}.qcow2

if [ "${NOAUTOCONSOLE}" = true ]; then
    NOAUTOCONSOLE="--noautoconsole"
else
    NOAUTOCONSOLE=
fi

if [ ${FILESYSTEM} != "none" ]; then
    if [ -d ${FILESYSTEM} ]; then
	FILESYSTEM="--memorybacking source.type=memfd,access.mode=shared --filesystem source.dir=${FILESYSTEM},target.dir=hostfs,driver.type=virtiofs"
    else
	echo "The specified FILESYSTEM (${FILESYSTEM}) is not a directory!"
	exit 1
    fi
else
    FILESYSTEM=
fi

if [ ${BRIDGE} != "none" ]; then
    NETWORK="--network bridge=${BRIDGE}"
else
    NETWORK="--network network:default"
fi

virt-install \
    --name ${NAME} \
    --osinfo ${OSINFO} \
    --cloud-init user-data=cloud-config-${NAME},network-config=network-config-${NAME} \
    --memory 1024 \
    --disk vol=default/${NAME}.qcow2,device=disk \
    ${FILESYSTEM} \
    --virt-type kvm \
    --graphics none \
    --console pty,target_type=serial \
    ${NOAUTOCONSOLE} \
    ${NETWORK} \
    --boot hd
